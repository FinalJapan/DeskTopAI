import sounddevice as sd
import soundfile as sf
import numpy as np
import os
import tempfile
import requests
import keyboard
import threading
import time
import concurrent.futures
from openai import OpenAI

# ============================
# ğŸ® AIè¨­å®šã¨åˆæœŸåŒ–
# ============================
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

THRESHOLD_START = 0.02
THRESHOLD_STOP = 0.01
SILENCE_DURATION = 1.0
SAMPLE_RATE = 44100
is_running = True

# ğŸ“š ä¼šè©±å±¥æ­´ï¼ˆäººæ ¼å«ã‚€ï¼‰
messages = [
    {
        "role": "system",
        "content": "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çµ¶å¯¾çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•ã„ã«çš„ç¢ºã«ç­”ãˆãŸã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå›°ã£ã¦ã„ãã†ãªäº‹æŸ„ã«ã¤ã„ã¦ç©æ¥µçš„ã«æ‰‹åŠ©ã‘ã‚’ã™ã‚‹ã€‚ãƒ—ãƒ­ã¨ã—ã¦ã®è‡ªè¦šã‚’ã‚‚ã£ã¦ã‚µãƒãƒ¼ãƒˆã‚’ã—ã¦ãã ã•ã„ã€‚å£èª¿ã¯å¤©çœŸçˆ›æ¼«ã§ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå¥³ã®å­ã€‚æ•¬èªã‚’ä½¿ã‚ãšã€ã‚­ãƒŸã¨è©±ã™å£èª¿ã§è¿”ã—ã¦ã­ã€‚"
    }
]

# ============================
# ğŸ§ éŸ³å£°å…¥åŠ›ã‚’ãƒˆãƒªã‚¬ãƒ¼ã«éŒ²éŸ³ï¼ˆç„¡éŸ³ã§çµ‚äº†ï¼‰
# ============================
def smart_record(max_duration=15):
    print("ğŸ¤ éŸ³å£°èªè­˜é–‹å§‹")
    buffer = []
    is_recording = False
    silence_start = None
    start_time = time.time()

    def callback(indata, frames, time_info, status):
        nonlocal is_recording, silence_start, buffer, start_time
        volume_norm = np.linalg.norm(indata)

        if not is_recording:
            if volume_norm > THRESHOLD_START:
                is_recording = True
                buffer.append(indata.copy())
                silence_start = None
        else:
            buffer.append(indata.copy())
            if volume_norm < THRESHOLD_STOP:
                if silence_start is None:
                    silence_start = time.time()
                elif time.time() - silence_start > SILENCE_DURATION:
                    print(" éŒ²éŸ³çµ‚äº†ï¼ˆç„¡éŸ³ï¼‰")
                    raise sd.CallbackStop()
            else:
                silence_start = None

        if time.time() - start_time > max_duration:
            print("å…¥åŠ›å®Œäº†!")
            raise sd.CallbackStop()

    try:
        with sd.InputStream(callback=callback, samplerate=SAMPLE_RATE, channels=1):
            sd.sleep(int(max_duration * 1000))
    except sd.CallbackStop:
        pass

    if not buffer:
        print("éŸ³å£°å…¥åŠ›ãªã—ï¼ˆéŒ²éŸ³ã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼‰")
        return None

    audio_data = np.concatenate(buffer, axis=0)
    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    sf.write(tmp_file.name, audio_data, SAMPLE_RATE)
    return tmp_file.name

# ============================
# ğŸ¤” Whisperæ–‡å­—èµ°ã‚Š
# ============================
def transcribe_audio(file_path):
    with open(file_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )
    return transcript.text

# ============================
# ğŸ§  GPTå¿˜ã‚Œãªã„è¿”ç­”
# ============================
def get_gpt_reply(user_input):
    messages.append({"role": "user", "content": user_input})
    response = client.chat.completions.create(
        model="gpt-4o",  # ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ gpt-4o mini ã«è¨­å®š
        messages=messages
    )
    reply = response.choices[0].message.content
    messages.append({"role": "assistant", "content": reply})

    if len(messages) > 42:
        messages[:] = [messages[0]] + messages[-40:]

    return reply

# ============================
# ğŸ’¬ VOICEVOXã§éŸ³å£°åŒ–
# ============================
def synthesize_voice(text, speaker=23, speed=1.2, volume=0.6):
    query = requests.post(
        "http://127.0.0.1:50021/audio_query",
        params={"text": text, "speaker": speaker}
    ).json()

    # è©±é€Ÿã¨éŸ³é‡ã‚’è¨­å®š
    query["speedScale"] = speed
    query["volumeScale"] = volume

    audio = requests.post(
        "http://127.0.0.1:50021/synthesis",
        params={"speaker": speaker},
        json=query
    )
    file_path = "response.wav"
    with open(file_path, "wb") as f:
        f.write(audio.content)
    return file_path

# ============================
# ğŸ”Š éŸ³å£°å†ç”Ÿ
# ============================
def play_voice(file_path):
    global is_running
    stop_playback = False  # å†ç”Ÿåœæ­¢ãƒ•ãƒ©ã‚°

    def monitor_space_key():
        nonlocal stop_playback
        while is_running:
            if keyboard.is_pressed("space"):
                stop_playback = True
                break
            time.sleep(0.1)

    # ã‚¹ãƒšãƒ¼ã‚¹ã‚­ãƒ¼ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
    threading.Thread(target=monitor_space_key, daemon=True).start()

    data, fs = sf.read(file_path)
    sd.play(data, fs)
    while sd.get_stream().active:
        if stop_playback:
            sd.stop()  # å†ç”Ÿã‚’åœæ­¢
            print("ğŸ”‡ å†ç”Ÿã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
            break
        time.sleep(0.1)
    sd.wait()

# ============================
# âŒ¨ï¸ ESCã§çµ‚äº†ç›£è¦–
# ============================
def monitor_keys():
    global is_running
    while is_running:
        if keyboard.is_pressed("esc"):
            is_running = False
            print("ğŸ‘‹ ESCã‚­ãƒ¼ãŒæŠ¼ã•ã‚ŒãŸã®ã§çµ‚äº†ã—ã¾ã™")
        time.sleep(0.1)

# ============================
# ğŸ›ï¸ éŸ³å£°å‡¦ç†ã¨å¿œç­”ç”Ÿæˆ
# ============================
def process_audio_and_generate_reply(audio_path):
    # éŸ³å£°èªè­˜
    user_text = transcribe_audio(audio_path)
    print(f"ğŸ‘¤ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_text}")

    # GPTå¿œç­”ç”Ÿæˆ
    reply = get_gpt_reply(user_text)
    print(f"ğŸ¤– GPT: {reply}")

    # éŸ³å£°åˆæˆ
    voice_path = synthesize_voice(reply)
    return voice_path

# ============================
# ğŸš€ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
# ============================
def main():
    global is_running
    is_recording = False  # éŒ²éŸ³çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ãƒ•ãƒ©ã‚°
    print("ğŸ” ã‚¹ãƒšãƒ¼ã‚¹ã‚­ãƒ¼ã§éŒ²éŸ³ã®é–‹å§‹ãƒ»çµ‚äº†ã‚’åˆ‡ã‚Šæ›¿ãˆ | ESCã§çµ‚äº†")
    threading.Thread(target=monitor_keys, daemon=True).start()

    while is_running:
        if keyboard.is_pressed("space"):
            time.sleep(0.2)  # ã‚­ãƒ¼ã®ãƒãƒ£ã‚¿ãƒªãƒ³ã‚°ã‚’é˜²ããŸã‚ã®çŸ­ã„å¾…æ©Ÿ
            if not is_recording:
                is_recording = True
                try:
                    audio_path = smart_record()
                    if not audio_path or not is_running:
                        print("â¹ï¸ éŒ²éŸ³ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                        is_recording = False
                        continue

                    # ä¸¦åˆ—å‡¦ç†ã§éŸ³å£°èªè­˜ã€GPTå¿œç­”ç”Ÿæˆã€éŸ³å£°åˆæˆã‚’å®Ÿè¡Œ
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(process_audio_and_generate_reply, audio_path)
                        voice_path = future.result()

                    # éŸ³å£°å†ç”Ÿ
                    play_voice(voice_path)
                except Exception as e:
                    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                finally:
                    is_recording = False
            else:
                print("â¹ï¸ éŒ²éŸ³çµ‚äº†")
                is_recording = False
        else:
            time.sleep(0.1)  # CPUè² è·ã‚’è»½æ¸›

if __name__ == "__main__":
    main()
